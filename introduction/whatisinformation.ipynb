{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aaef73-dac8-4f56-968f-e83517262260",
   "metadata": {},
   "source": [
    "# What is information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f0cc3-58df-4123-89d7-1d232c3e8383",
   "metadata": {},
   "source": [
    "Why do you ask a question? That is because you do not know something and try to find it.  In other words, you try to get new [information](https://en.wikipedia.org/wiki/Information).  But what is information and can we quantify the amount of information?  To answer this question, [information theory](https://en.wikipedia.org/wiki/Information_theory) was developed in the first half of twenty century by [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) and others.  It turns out that  the theory of information is directly related to the theory of probability and also thermodynamics. At the end, we understand that information dictates all physical processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b12c3-3096-4a1c-85a0-6a7864fc9f92",
   "metadata": {},
   "source": [
    "## Classical information\n",
    "\n",
    "Consider coin flipping.  It has two possible outcomes, head and tail. Before tossing it up, you don't  knows the outcome.  This uncertainty is the origin of information. At the moment the outcome becomes known to you, the uncertainty vanishes.  It remains vanished until you forget the outcome.  To mathematically describe the information you gained, we need three things.\n",
    "\n",
    "1. A set of all possible outcomes, $\\{\\omega_1, \\omega_2, \\cdots, \\omega_N\\}$.\\ where $N$ is the total number of possible outcomes.\n",
    "2. The probability $\\omega_i$ that the outcome $\\omega_i$ is obtained.\n",
    "3. The uncertainty is measured by\n",
    "$$\n",
    "H = - \\sum_i^N p_i \\log p_i.\n",
    "$$\n",
    "which is known as *Shannon entropy* or *information entropy*,\n",
    "\n",
    "\n",
    "In the above example, the possible outcomes before the tossing are $\\omega_1$=head and $\\omega_2$=tail.  Thus $N=2$.  Assuming that head and tail are equally likely, $p_1=p_2=\\frac{1}{2}$.  The amount of uncertainty is $H=\\log 2$.  Let us assume that the outcome is head.  You examine the outcome again.  The possible outcome of the examination is head.  Hence, now the number of possible outcome is $N'=1$ and the probability is $p^\\prime_1$=1.  The uncertainty vanished since $H'=0$.  The amount of change in the uncertainty $\\Delta H = H-H'$ is the amount of information you gained.\n",
    "\n",
    "The value of the logarithmic function depends on a choice of base. We use base 2 or $e$ depending of the situation.  We use $\\ln$ for base $e$.  When the base 2 is used, the amount of information is measured in *bits*.  For the coin tossing, 1 bit of information is obtained since $\\log_2 2 = 1$.  $N$ bits of information corresponds to $2^N$ different outcomes.\n",
    "\n",
    "Let us consider another example:   When you role a die, only one of the six possible faces is observed. Hence, $N=6$ and the possible outcome is one of $\\omega_1=1, \\cdots, \\omega_6 = 6$.  The probability to obtain each face is $p_1 = \\cdots = p_6 = \\frac{1}{6}$.  Hence the amount of gained information is $\\Delta H = \\log 6 \\approx 2.58$.  Notice that you gain more information  in rolling a die than in tossing a coin. This is more uncertainty in the former than the latter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433fa5e-28a8-4ec3-a1e8-b12d6900285c",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "Exercise \n",
    "The size of hard disk is usually measured in *bytes*.  One byte is just 8 bits.  Hence, 1TB of hard disk can store $8 \\times 10^12$ bits of information.  Find how many different data can be stored in it?\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad76f71-747f-4997-a33d-f2562c6ea938",
   "metadata": {},
   "source": [
    "## Quantum Information\n",
    "\n",
    "Quantum computers store information in quantum states, which are vector as we discuss in next sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
